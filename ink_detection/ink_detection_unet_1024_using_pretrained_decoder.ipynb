{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class LazyLoadDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # To store tuples of (X_paths, Y_path)\n",
    "\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                ink_label_dir = os.path.join(class_dir, \"inklabel_crops\")\n",
    "                surface_volume_dir = os.path.join(class_dir, \"surface_volume\")\n",
    "\n",
    "                for i in range(65):  # Assuming there are 65 crop images\n",
    "                    Y_path = os.path.join(ink_label_dir, f\"crop_{i}.png\")\n",
    "                    X_paths = [\n",
    "                        os.path.join(surface_volume_dir, f\"{j:02d}_crops\", f\"crop_{i}.png\")\n",
    "                        for j in range(65)\n",
    "                    ]\n",
    "                    self.samples.append((X_paths, Y_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_paths, Y_path = self.samples[idx]\n",
    "\n",
    "        return X_paths, Y_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X paths: [('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\00_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\01_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\02_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\03_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\04_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\05_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\06_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\07_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\08_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\09_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\10_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\11_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\12_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\13_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\14_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\15_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\16_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\17_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\18_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\19_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\20_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\21_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\22_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\23_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\24_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\25_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\26_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\27_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\28_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\29_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\30_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\31_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\32_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\33_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\34_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\35_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\36_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\37_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\38_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\39_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\40_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\41_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\42_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\43_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\44_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\45_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\46_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\47_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\48_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\49_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\50_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\51_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\52_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\53_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\54_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\55_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\56_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\57_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\58_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\59_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\60_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\61_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\62_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\63_crops\\\\crop_32.png',), ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\64_crops\\\\crop_32.png',)]\n",
      "Y path: ('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\inklabel_crops\\\\crop_32.png',)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "root_dir = \"../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\"\n",
    "transform = None  # You can add transformations if needed\n",
    "\n",
    "dataset = LazyLoadDataset(root_dir=root_dir)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in data_loader:\n",
    "    X_paths, Y_path = batch\n",
    "    print(\"X paths:\", X_paths)\n",
    "    print(\"Y path:\", Y_path)\n",
    "    break  # Print the first batch for demonstration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "('../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\surface_volume\\\\64_crops\\\\crop_32.png',)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_paths[64]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'../../Datasets/vesuvius-challenge-ink-detection/cropped_train_1024\\\\3\\\\inklabel_crops\\\\crop_32.png'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_path[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}